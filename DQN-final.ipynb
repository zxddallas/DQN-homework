{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 用深度强化学习玩Atari游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 深度Q网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在$Q$学习算法中，若状态空间与动作空间是离散且维度不高时，可以使用$Q$表储存每个状态动作对应的Q值。当状态空间是高维空间时，由于状态太大使用表格式方法变得十分困难甚至无法完成。在这种情况，将Q表的更新转换为函数拟合问题，通过拟合一个函数代替Q表来估计Q值，使得相近状态下输出相近的动作。用于替代Q表的函数被称为动作值函数，通常简单称其为Q函数。\n",
    "\n",
    "因此，在状态空间规模非常大时，通常将Q表的更新转换为函数拟合问题。由于神经网络的表达能力强，是用于拟合动作值函数良好选择。因此可以使用深度神经网络来拟合动作值函数来代替Q表产生Q值，使得相近的状态得到相近的输出动作。\n",
    "\n",
    "深度学习与强化学习会存在以下问题：\n",
    "\n",
    "- 深度学习是监督学习需要训练集，而强化学习不需要训练集，只通过环境返回回报值；同时也存在着噪声和延迟的问题；也存在很多状态的回报值都是0，也即样本稀疏的问题；\n",
    "- 深度学习中通常每个样本之间是相互独立的，而强化学习中当前状态的状态值依赖后面的状态返回值；\n",
    "- 使用非线性网络来表示值函数可能出现不稳定。\n",
    "\n",
    "\n",
    "使用深度神经网络拟合动作值函数近似的方法称为**深度Q网络**（**Deep Q Network, DQN**）。在DQN中，对上述问题的解决方案为：\n",
    "\n",
    "- 通过Q学习使用回报来构造标签；\n",
    "- 通过**经验回放**（**experience replay**）来解决样本状态相关性以及非静态分布问题；\n",
    "- 使用一个网络产生当前的Q值，而使用另一个目标网络产生对应的目标Q值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 状态动作值函数近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "状态动作值函数$Q(\\cdot, \\cdot)$的输入为状态$s$，输出 $Q(s,a),\\forall a \\in A$，$A$ 是动作空间。根据状态动作值函数，使用 **$\\epsilon$-贪婪** 策略来选择动作。训练时，环境会产生观测，智能体根据状态动作值函数得到该观测的Q值，然后应用上述策略确定动作，环境接收到此动作后会反馈奖励及下一个观测。对于$\\epsilon$-贪婪策略可以形式化的描述如下：\n",
    "$$a_t = \\begin{cases} \\mathrm{rand(A)} & \\text{if } p < \\epsilon, \\\\\n",
    "\\arg \\max_{a'} Q(s_t, a') & \\text{otherwise}, \\end{cases}$$\n",
    "其中$p$是一个随机数，$\\mathrm{rand(A)}$指从动作空间$A$中随机选择一个动作。\n",
    "\n",
    "状态动作值函数神经网络的训练需要定义真值。由于环境模型未知，因此无法得到状态动作值函数的真值。在这种情况下，获得状态动作值函数的方法一般是随机采样，如蒙特卡罗采样，将每一**幕**场景（**episode**）运行一遍，然后采样得到各个状态的值函数。有终止状态的场景称为分幕式场景，其中一幕指的是从某个初始状态到终止状态的整个序列，如一局游戏从开始到结束。\n",
    "\n",
    "最优动作值函数遵守强化学习中称为Bellman方程的等式。直观地说，如果序列$s'$在下一时间步的最优值$Q^{*}(s', a')$对所有可能动作$a'$已知，最优策略是选择使得$r+\\gamma Q^{*}(s', a')$期望值最大的动作$a'$：\n",
    "$$Q^{*}(s, a) = \\mathbb{E}_{s'} \\Big[ r + \\gamma_{a'} \\max_{a'} Q^{*}(s', a') \\big| s, a \\Big].$$\n",
    "\n",
    "许多强化学习算法背后的基本思想都是基于Bellman方程以迭代更新的方式估计动作值函数：\n",
    "$$Q_{i+1} (s, a) = \\mathbb{E}_{s'} \\Big[ r + \\gamma_{a'} \\max_{a'} Q_{i}^{*}(s', a') \\big| s, a \\Big].$$\n",
    "该值迭代算法当$i \\to \\infty$时收敛到最优动作值函数，即$Q_i \\to Q^{*}$。然而在实践中，这个基本方法不实用，因为动作值函数是对每一个序列单独估计的，不具备推广性。作为替代方案，常用的方法是使用函数近似来估计动作值函数，即$Q(s, a; w) \\approx Q^{*}(s, a)$，式中的$w$是函数的参数。\n",
    "\n",
    "深度Q网络可以通过最小化Bellman方程的均方误差来调整第$i$次迭代的参数$w_i$，最优目标值$r + \\gamma \\max_{a'} Q^{*}(s', a')$使用近似的目标值$y = r + \\gamma \\max_{a'} Q(s', a'; w_{i}^{-})$来替代，其中使用的参数$w_{i}^{-}$来自前面某轮迭代。这就引出了在每次迭代$i$中不断变化的损失函数序列$L_i(w_i)$：\n",
    "$$L_i(w_i) = \\mathbb{E}_{s, a, r} \\Big[ \\big( \\mathbb{E}_{s'}[y|s, a] - Q(s, a; w_i) \\big)^{2} \\Big].$$\n",
    "\n",
    "在Q学习中，使用 $\\epsilon$-贪婪策略来生成动作 $a_{t+1}$；但用来计算状态动作值函数的是使得 $Q(s_{t+1}, a_{t+1})$ 最大的动作。这种产生行为的策略和进行评估的策略不同的方法称为**异策略**（**off-policy**）方法。DQN中也使用了异策略方法。不同的是，Q学习中用来计算目标和预测值的Q是同一个Q，即使用了相同的神经网络。这样带来的一个问题就是，每次更新神经网络时，目标网络也被更新，容易导致神经网络参数不收敛。因此DQN在原本的 Q 网络的基础上引入了一个目标Q网络，用来计算目标值。它和Q网络结构一样，初始的权重也一样，只是Q网络每次迭代都会更新，而目标Q网络每隔一段时间才会更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 经验回放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Q学习是一种异策略的方法，既可以学习当前经验也可以学习过去经验。因此在学习过程中随机地加入之前的学习经验会让神经网络的训练更有效率，**经验回放**(experience replay)缓冲区记录的就是过去的学习经历。\n",
    "\n",
    "经验回放解决了相关性问题以及非静态分布的问题。通过在每个时刻智能体与环境交互得到的状态转移样本 $(s_t, a_t, r_t, s_{t+1})$ 储存到回放缓冲区，在训练时就随机拿出来一个批次的样本，这样可以打乱状态之间的相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 算法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 首先初始化经验回放缓冲区D，它的容量为N;\n",
    "2. 初始化Q网络，随机生成权重 $w$;\n",
    "3. 初始化目标Q网络，权重为 $w^- = w$;\n",
    "4. 循环遍历 $\\mathrm{episode} = 1, 2,\\dots, M$:\n",
    "5. 初始化初始状态$s_1$;\n",
    "6. 循环遍历 $t = 1,2,\\dots, T$:\n",
    "    - 用 $\\epsilon$−贪婪策略生成动作 $a_t$：以 $\\epsilon$ 概率选择一个随机的动作，或选则动作$a_t = \\max_{a} Q(s_t,a;w^-)$;\n",
    "    - 执行动作 $a_t$，接收奖励 $r_t$ 及新的状态 $s_{t+1}$;\n",
    "    - 将样本 $(s_t, a_t, r_t, s_{t+1})$ 存入 $D$ 中；\n",
    "    - 从 $D$ 中随机抽取一个 mini-batch 的数据 $\\{ (s_j, a_j, r_j, s_{j+1})_k \\}_{k=1}^{K}$；\n",
    "    - 令 $y_j=r_j$，如果 $j+1$ 步是终止的话，否则，令 $y_j = r_j+\\gamma \\max_{a'}Q(s_{t+1},a'; w^-)$；\n",
    "    - 对 $[y_j − Q(s_t,a_j; w)]^2$ 关于 $w$ 使用梯度下降法进行更新；\n",
    "    - 每隔 $C$ 步更新目标Q 网络 $w^- = w$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 算法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "我们将在这里复现深度强化学习中的DQN模型，论文原文：[Human-level Control Through Deep Reinforcement Learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)。模型接收游戏的图像作为输入，采用端到端的模型直接预测下一步要执行的控制，本项目需要在GPU环境下运行。\n",
    "\n",
    "这里的实现主要参考了百度AI Studio项目 <https://aistudio.baidu.com/aistudio/projectdetail/169455>。考虑到教学内容的需要，对代码进行了一定的修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基于PaddlePaddle的深度Q网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "深度Q网络在文件`DQN_agent.py`文件中实现为类`DQNModel`。该深度卷积神经网络的输入包括了状态(`state`)，动作(`action`)，回报(`reward`)，下一个状态(`next_s`)以及游戏是否结束(`isOver`)的信息。这里实现的神经网络结构包括四个卷积(`conv`)池化(`max pool`)层接一个全连接(`fc`)层。你可以根据你所学的深度学习知识进行修改。\n",
    "\n",
    "下面是该类中各函数的作用：\n",
    "\n",
    "- `__init__`：初始化类实例；\n",
    "- `_get_inputs`：对网络的输入数据进行处理，使之适合于PaddlePaddle框架；\n",
    "- `_build_net`：建立深度神经网络，包括了三个不同的执行程序，预测程序（对应于目标Q网络）、训练程序（对应于学习的Q网络）、以及同步程序（同步Q网络参数至目标Q网络）；\n",
    "- `get_DQN_prediction`：计算目标Q网络的预测；\n",
    "- `act`：根据状态输入选择合适的动作；\n",
    "- `train`：训练神经网络；\n",
    "- `sync_target_network`：向目标Q网络同步参数；\n",
    "- `save_inference_model`：将训练得到的网络参数存储到指定目录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 经验回放缓冲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "经验回放缓冲区在文件`expreplay.py`中实现。其中`Experience`定义为状态、动作、回报以及游戏是否结束的四元组。而类`ReplayMemory`用于管理这些经验样本。经验样本被存放于一个循环队列中，当队列填充满后，新的经验将覆盖旧的经验样本。\n",
    "\n",
    "类`ReplayMemory`中的主要函数功能描述如下：\n",
    "\n",
    "- `append`：向缓冲区尾填充经验样本；\n",
    "- `recent_state`：获取最近的状态；\n",
    "- `sample`：根据指定序号获取经验样本；\n",
    "- `sample_batch`：获取一个小批次(mini-batch)的经验样本，用于训练神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 深度Q学习的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "首先安装并导入需要使用到的Python程序包。各个程序包的作用为：\n",
    "\n",
    "- `gym`：强化学习的训练场，用于模拟各种环境，本次实验中用于模拟Atari 2600视频游戏；\n",
    "- `atari_py`：模拟Atari 2600视频游戏，与`gym`一起提供强化学习环境；\n",
    "- `expreplay`：经验回放缓冲区；\n",
    "- `DQN_agent`：实现DQN模型；\n",
    "- `numpy`：科学计算中的各种数组及其基本运算；\n",
    "- `os`：与系统交互，文件路径管理；\n",
    "- `tqdm`：显示训练的进度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "由于百度AI Studio环境没有`gym`和`tqdm`，因此需要在第一次运行本项目时执行下述脚本进行安装。包安装成功后，请重启执行器，否则无法导入新安装的程序包。\n",
    "\n",
    "请去掉下面单元格开头的`#`，执行单元格，包安装成功再加上`#`，然后从菜单栏重启执行器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: gym[atari]==0.15.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.15.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (4.36.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (1.15.0)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (4.1.1.26)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (1.16.4)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (1.3.0)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (7.1.2)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym[atari]==0.15.4) (0.2.5)\n",
      "Requirement already satisfied: future in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]==0.15.4) (0.18.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: atari-py==0.2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.2.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from atari-py==0.2.5) (1.16.4)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from atari-py==0.2.5) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install gym[atari]==0.15.4 tqdm\r\n",
    "#!pip install atari-py==0.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "导入需要使用的类和库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "from expreplay import ReplayMemory, Experience\n",
    "from DQN_agent import DQNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "定义函数中需要用到的一些参数。这些参数的含义将在后面用到的时候介绍。这部分参数通常使用目前给定的值而无需修改，你也可以尝试修改部分值，探索它们对算法性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 1e6\n",
    "MEMORY_WARMUP_SIZE = MEMORY_SIZE // 20\n",
    "IMAGE_SIZE = (84, 84)\n",
    "CONTEXT_LEN = 4\n",
    "GAMMA = 0.99\n",
    "\n",
    "ACTION_REPEAT = 4  # aka FRAME_SKIP\n",
    "UPDATE_FREQ = 4\n",
    "SYNC_TARGET_FREQ = 10000 // UPDATE_FREQ\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基本辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面定义两个函数，用于辅助实现$\\epsilon$-贪婪策略。\n",
    "\n",
    "- `action_random`：从动作空间中随机选择一个动作；\n",
    "- `action_policy`：根据已经学习获得的目标Q网络选择一个已经习得的策略意义下的最优动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def action_random(env):\n",
    "    action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def action_policy(agent, state, exp):\n",
    "    context = exp.recent_state()\n",
    "    context.append(state)\n",
    "    context = np.stack(context, axis=0)\n",
    "    action = agent.act(context)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面的函数`train_agent`的功能是从经验回放缓冲区中随机抽取`BATCH_SIZE`个样本，用于训练智能体的策略网络（深度Q网络）。由于视频游戏一帧图像缺少运动信息，会导致控制策略学习的困难。为解决这个问题，将连续`CONTEXT_LEN`帧图像合并在一起作为深度Q网络的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_agent(agent, exp):\n",
    "    batch_all_state, batch_action, batch_reward, batch_isOver = exp.sample_batch(BATCH_SIZE)\n",
    "    batch_state = batch_all_state[:, :CONTEXT_LEN, :, :]\n",
    "    batch_next_state = batch_all_state[:, 1:, :, :]\n",
    "    agent.train(batch_state, batch_action, batch_reward,\n",
    "                batch_next_state, batch_isOver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "函数`eval_agent`生成新的游戏以评估当前学到的目标Q网络的性能。为了得到性能的准确估计，该函数使用测试环境(`test_env`)玩`n_episodes`局（幕）游戏，取其平均回报。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_agent(agent, env, n_episodes=32):\n",
    "    episode_reward = []\n",
    "    for _ in range(n_episodes):\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = agent.act(state)\n",
    "            state, reward, isOver, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if isOver:\n",
    "                break\n",
    "        episode_reward.append(total_reward)\n",
    "    eval_reward = np.mean(episode_reward)\n",
    "    return eval_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 分幕式学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "函数`train_episode`使用一局游戏，或称为一幕数据，来训练深度Q网络。该函数是整个算法最为核心的部分。在开始，对环境进行重置启动游戏，并循环赶到游戏结束。在每一步中，首先执行$\\epsilon$-贪婪策略来选择动作。执行动作后收集训练样本，放入经验回放缓冲区。在训练的前面部分，$\\epsilon$(`g_epsilon`)值不断减小，以减少随机探索产生的样本。\n",
    "\n",
    "每隔`UPDATE_FREQ`步，从经验回放缓冲区中提取样本训练深度Q网络。每训练`SYNC_TARGET_FREQ`个批次，将Q网络的参数同步到目标Q网络。若某一步后游戏结束，则该幕（局）的训练结束。\n",
    "\n",
    "请你补充$\\epsilon$-贪婪策略的实现代码。即分别把\n",
    "\n",
    "- `action = action_policy(agent, state, exp)`\n",
    "- `action = action_random(env)`\n",
    "\n",
    "放到注释块“epsilon 贪婪”的合适位置。\n",
    "\n",
    "请你补充训练智能和同步网络的代码，即分别把\n",
    "\n",
    "- `agent.sync_target_network()`\n",
    "- `train_agent(agent, exp)`\n",
    "\n",
    "放到注释块“epsilon 贪婪”的合适位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_episode(agent, env, exp, warmup=False):\n",
    "    global g_epsilon\n",
    "    global g_train_batches\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        step += 1\n",
    "        # epsilon greedy action\n",
    "        prob = np.random.random()\n",
    "        # ======= 将 epsilon 贪婪 代码补充到这里\n",
    "        if prob < g_epsilon:\n",
    "            action = action_random(env)\n",
    "        else:\n",
    "            action = action_policy(agent,state,exp)\n",
    "        # ======= 补充代码结束\n",
    "        next_state, reward, isOver, _ = env.step(action)\n",
    "        exp.append(Experience(state, action, reward, isOver))\n",
    "        g_epsilon = max(0.1, g_epsilon - 1e-6)\n",
    "\n",
    "        # train model\n",
    "        if not warmup and len(exp) > MEMORY_WARMUP_SIZE:\n",
    "            # ======= 将 训练智能体 代码补充到这里\n",
    "            if step % UPDATE_FREQ == 0:\n",
    "                train_agent(agent,exp)\n",
    "                if g_train_batches % SYNC_TARGET_FREQ == 0:\n",
    "                    agent.sync_target_network()\n",
    "                g_train_batches += 1\n",
    "            # ======= 补充代码结束\n",
    "    \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if isOver:\n",
    "            break\n",
    "    return total_reward, step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 创建游戏环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在Atari 2600视频游戏中，环境给出的游戏状态是 210×160 的图像，每个像素有128种可能的颜色。这是一个非常大的输入空间。为了降低复杂度，我们将图像转换为灰度，并将其大小调整为为84×84。该操作可以使用`gym`封装的类`AtariPreprocessing`很方便的使用。我们选择的游戏的例子是`Pong`，我们需要输入环境名`PongNoFrameskip-v0`，其中`NoFrameskip`表示没有跳帧，这是使用`AtariPreProcessing`的约束。\n",
    "\n",
    "你可以下载视频`DQN-Pong.avi`了解一下`Pong`游戏。\n",
    "\n",
    "下面首先创建用于训练深度Q网络的环境`env`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_name = 'PongNoFrameskip-v0'\n",
    "env = gym.make(env_name)\n",
    "# env = FireResetEnv(env)\n",
    "env = AtariPreprocessing(env)\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来创建用于评估当前目标Q网络性能的测试环境`test_env`。由于环境每次仅产生一帧图像，而深度Q网络输入为连续`CONTEXT_LEN`帧图像，这里使用封装类`FrameStack`来累积图像。在训练环境中，图像累积是由经验回放缓冲区实现的，因此无需使用`FrameStack`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_env = gym.make(env_name)\n",
    "# test_env = FireResetEnv(test_env)\n",
    "test_env = AtariPreprocessing(test_env)\n",
    "test_env = FrameStack(test_env, CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "你也可以尝试其它游戏。深度Q网络的优点是你无需关注游戏的细节知识，可以直接使用目前的框架进行学习就可以在大多数游戏上取得很好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练深度Q网络控制Atari游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "首先初始化主要的变量$\\epsilon$-贪婪策略参数`g_epsilon`以及控制同步目标Q网络参数的变量`g_train_batches`。\n",
    "\n",
    "生成经验回放缓冲区`exp`，该缓冲区可以容纳`MEMORY_SIZE`幅大小为`IMAGE_SIZE`的图像。\n",
    "\n",
    "最后创建基于深度Q网络的智能体`agent`，模型处理的图像大小为`IMAGE_SIZE`，动作空间维度为`action_dim`，折扣回报系数为`GAMMA`。\n",
    "\n",
    "参数`USE_CUDA`表明是否使用GPU进行训练。由于该算法复杂度高，需要使用GPU进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 00:13:24.116722  1339 device_context.cc:259] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0\n",
      "W1208 00:13:24.122500  1339 device_context.cc:267] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "g_epsilon = 1.1\n",
    "g_train_batches = 0\n",
    "\n",
    "exp = ReplayMemory(int(MEMORY_SIZE), IMAGE_SIZE, CONTEXT_LEN)\n",
    "agent = DQNModel(IMAGE_SIZE, action_dim, GAMMA, CONTEXT_LEN, USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用随机策略进行`MEMORY_WARMUP_SIZE`步游戏对经验回放缓冲区进行热身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memory warmup: 50500it [00:58, 864.41it/s]                             \n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=MEMORY_WARMUP_SIZE, desc='Memory warmup')\n",
    "while len(exp) < MEMORY_WARMUP_SIZE:\n",
    "    total_reward, step = train_episode(agent, env, exp, warmup=True)\n",
    "    pbar.update(step)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "定义训练使用游戏总步数`TOTAL_STEPS`（注意不是局数）。定义在训练中期望评估目标Q网络的次数`N_EVALS`。注意这里的`N_EVALS`与前面的`n_episodes`的差异。\n",
    "\n",
    "在达到总的游戏步数前，反复执行分幕式学习以训练深度Q网络。每隔`TEST_EVERY_STEPS`步评估一次目标Q网络的性能，若其平均回报优于此前学到的最佳模型，则将网络参数使用`save_inference_model`记录下来。持久化的模型存储在目录`saved_models`目录下，每个模型一个文件夹，且文件夹的名称包含了第几次评估及对应的平均回报值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 提示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 在执行以下程序时，建议先**跳转到思考题**部分，根据需要完成的思考题来调整程序参数。\n",
    "- 由于AI Studio的环境原因，`tqdm`的进度条可能会出现不断换行的问题。重启执行器可以避免这个问题，但是训练过程需要从头开始。\n",
    "- 由于训练时间很长，需要使用GPU进行训练，请设置`USE_CUDA=True`。目前，每天运行一次AI Studio项目，百度会赠送10小时GPU训练时间(GPU算力卡点数)。\n",
    "- 如果在训练过程中你需要关闭浏览器（或计算机），请检查AI Studio中左侧工具栏中的“**设置**”，将“**当您关闭当前页面时, 您希望后台环境保持运行到**:”选择为你期望的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DQN agent: 1552110it [16:41:09, 25.84it/s, stage=EVAL 101 Reward 2.84]                                \n"
     ]
    }
   ],
   "source": [
    "# ======= 请修改以下参数到你期望的值\n",
    "N_EVALS = 100\n",
    "TOTAL_STEPS = 1550000\n",
    "# ======= 参数修改结束\n",
    "TEST_EVERY_STEPS = TOTAL_STEPS // N_EVALS\n",
    "\n",
    "test_flag = 0\n",
    "pbar = tqdm(total=TOTAL_STEPS, desc='Training DQN agent')\n",
    "total_step = 1\n",
    "max_reward = None\n",
    "while total_step < TOTAL_STEPS:\n",
    "    # start epoch\n",
    "    pbar.set_postfix(stage='TRAIN', epsilon=g_epsilon, refresh=False)\n",
    "    total_reward, step = train_episode(agent, env, exp)\n",
    "    total_step += step\n",
    "\n",
    "    if total_step // TEST_EVERY_STEPS == test_flag:\n",
    "        test_flag += 1\n",
    "        pbar.set_postfix(stage=\"EVAL {}\".format(test_flag), refresh=False)\n",
    "        eval_reward = eval_agent(agent, test_env)\n",
    "        pbar.set_postfix(stage=\"EVAL {} Reward {:.2f}\".format(test_flag, eval_reward), refresh=False)\n",
    "\n",
    "        if max_reward is None or eval_reward > max_reward:\n",
    "            max_reward = eval_reward\n",
    "            model_folder = '{}-{}-{}_{:.2f}'.format('DQN', 'Pong', test_flag, eval_reward)\n",
    "            save_path = os.path.join('saved_models', model_folder)\n",
    "            agent.save_inference_model(save_path)\n",
    "    pbar.update(step)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 用训练好的智能体玩Atari游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "脚本`play.py`提供了使用保存的DQN模型玩Atari游戏并将其保存为视频的功能。请使用如下命令行使用保存的智能体模型玩一下Pong游戏：\n",
    "```bash\n",
    "!python3 play.py --use_cuda --game {env_name} --model_path {model_path} --viz\n",
    "```\n",
    "其中`{env_name}`为游戏对应的环境名称，这里我们使用的是`PongNoFrameskip-v0`。`{model_path}`对应的是保存模型的路径，一个可能的路径为`saved_models/DQN-Pong-46-0.62`。`--viz`表示是否要生成视频，不加`--viz`参数将仅报告玩游戏的回报；`--viz`生成的视频会保存在`videos`目录下，请下载查看。参数`--use_cuda`表示在模型推理过程中是否使用GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1208 17:13:53.848743  7803 device_context.cc:259] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.1, Runtime API Version: 9.0\n",
      "W1208 17:13:53.854593  7803 device_context.cc:267] device: 0, cuDNN Version: 7.6.\n",
      "eval agent: 100%|█████████████████████████████████| 1/1 [00:13<00:00, 13.24s/it]\n",
      "Average reward of epidose: 13.0\n"
     ]
    }
   ],
   "source": [
    "!python3 play.py --use_cuda --game PongNoFrameskip-v0 --model_path saved_models/DQN-Pong-100_10.28 --viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 思考题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 评估模型`eval_agent`得到的回报值意味着什么？\n",
    "    - 结合`Pong`游戏的特点（请下载查看视频`DQN-Pong.avi`），说明为什么刚开始训练时游戏的回报是负值？\n",
    "    - 是否平均回报值达到21时，智能体才能赢得游戏？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：1.评估模型eval_agent得到的回报意味着在当前模型进行若干次游戏的平均回报，用来衡量当前模型的好坏程度。\n",
    "\n",
    "2.刚开始回报是负值是由于模型参数较差，“智能体”状态不智能。\n",
    "\n",
    "3.并不是，智能体只需要先达到21次进球即可获胜，因此平均回报达到 1 即可视为赢得游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. 训练的总步数`TOTAL_STEPS`对学到的模型性能有什么影响？请尝试增加`TOTAL_STEPS`的值，得到更好的模型。提示：在`Pong`游戏上，DQN模型的回报有可能达到21。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：TOTAL_STEPS表示训练中环境进行的总步数，TOTAL_STEPS越大，训练时间越长，模型效果也可能越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. 是否可以使用玩游戏的幕数来控制训练的过程，控制总步数与控制游戏幕数两者有何差异？结合DeepMind训练打砖块智能体使用的训练幕数来估计训练出达到人类玩家水平的智能体的总步数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：可以；控制总步数更倾向于每一步的训练，而控制游戏幕数侧重整局游戏的训练。通过控制总步数，可使智能体学会在当前状态对未来的某些策略做考虑；达到人类玩家的总步数在600000步左右"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. 你最终训练得到的智能体在评估阶段的平均回报是多少？请使用你获得的最优模型生成玩游戏的视频。请下载你所得到的最优模型的参数以及用它玩游戏的视频，并上传到项目文件夹下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. 函数`train_episode`中有一句\n",
    "\n",
    "```python\n",
    "        g_epsilon = max(0.1, g_epsilon - 1e-6)\n",
    "```\n",
    "\n",
    "- 它的作用是什么？\n",
    "- 如果去掉这句，学习算法是否还能够收敛，为什么？\n",
    "- $\\epsilon$的每步减小量$10^{-6}$有什么含义吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：1.一是限制g_epsilon值的大小，避免出现没有随机的情况；二是让随机性随着训练不断下降，以便让算法更快的收敛下来。\n",
    "\n",
    "2.有可能，只是概率会降低，这会增加对初始g_epsilon的要求，如果太小的话就会缺失随机性，太大的话不能稳定，最终无法收敛\n",
    "\n",
    "3.含义是十万步后减小到最小值0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. 模型评估对所学模型性能的影响\n",
    "    - `N_EVALS`值对整个训练过程所需要的时间有何影响？\n",
    "    - `N_EVALS`值对于所学到的模型的平均回报是否有影响，为什么？\n",
    "    - 执行`eval_agent`的时机对于所学到的模型性能是否有影响，为什么？\n",
    "    - 是否有更好的方法选择评估模型的时机？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：1.有影响。N_EVALS越大代表验证的次数越多，消耗的总训练时间也越多。\n",
    "\n",
    "2.无影响，因为验证时模型参数并未发生改变。\n",
    "\n",
    "3.有影响，执行的时机可能会打断训练过程中的游戏。\n",
    "\n",
    "4.可以选择在游戏结束时进行评估模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 附加题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. 算法记录了评估过程中不断改善的各个模型，尝试使用所记录的模型，对比不同平均回报的模型玩游戏的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：通过观察和对比发现，不同模型的差距主要都体现在接到球的概率上；平均回报越大的模型，智能体接到球的概率越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. 游戏`Pong`需要执行`FIRE`动作才会开始。如果不执行该动作，游戏会停止在没有发球的阶段。类`FireResetEnv`封装了强制环境产生第一个动作为`FIRE`的游戏动作序列。\n",
    "\n",
    "    - 使用和不使用`FireResetEnv`对DQN学习有何影响，为什么？\n",
    "\n",
    "```python\n",
    "class _FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "def FireResetEnv(env):\n",
    "    if isinstance(env, gym.Wrapper):\n",
    "        baseenv = env.unwrapped\n",
    "    else:\n",
    "        baseenv = env\n",
    "    if 'FIRE' in baseenv.get_action_meanings():\n",
    "        return _FireResetEnv(env)\n",
    "    return env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：使用FireResetEnv会让训练更稳定，收敛性更佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9. 折扣回报系数`GAMMA`的值对算法性能有什么影响？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：GAMMA越大，智能体越考虑长远的收益，同时算法会更注重整体收益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "10. 尝试使用同样的模型学习能够玩打砖块游戏的智能体。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "11. 对于第一次作业中的出租车问题，能够使用DQN模型来解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "答：可以，该问题状态空间和动作空间同Pong一样，分别是Box(210, 160, 3)，Discrete类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用`gym`的代码片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 列出游戏环境的观测空间、动作空间以及各个动作的含义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(6)\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v4')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "Discrete(4)\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v4')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 列出`gym`支持的环境名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pong-v0',\n",
       " 'Pong-v4',\n",
       " 'PongDeterministic-v0',\n",
       " 'PongDeterministic-v4',\n",
       " 'PongNoFrameskip-v0',\n",
       " 'PongNoFrameskip-v4',\n",
       " 'Pong-ram-v0',\n",
       " 'Pong-ram-v4',\n",
       " 'Pong-ramDeterministic-v0',\n",
       " 'Pong-ramDeterministic-v4',\n",
       " 'Pong-ramNoFrameskip-v0',\n",
       " 'Pong-ramNoFrameskip-v4']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [env.id for env in gym.envs.registry.all()]\n",
    "list(filter(lambda x: x.find('Pong') >= 0, names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 使用`Monitor`录制视频"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用`Monitor`是录制视频最便捷的方案。由于百度AI Studio环境中没有提供`ffmpeg`，因此只能使用`opencv`提供的功能以编程方式实现图像到视频的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "env_name = 'Pong-v0'\n",
    "env = gym.make(env_name)\n",
    "env = Monitor(env, './monitor', force=True)\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(1024):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render(mode='rgb_array')\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 aistudio aistudio 60191 Dec  8 17:02 monitor/openaigym.video.0.1339.video000000.mp4\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l monitor/*.mp4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
